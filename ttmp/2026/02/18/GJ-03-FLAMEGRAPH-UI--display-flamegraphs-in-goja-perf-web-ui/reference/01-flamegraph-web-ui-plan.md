---
Title: Flamegraph Web UI Plan
Ticket: GJ-03-FLAMEGRAPH-UI
Status: active
Topics:
    - ui
    - tooling
    - goja
DocType: reference
Intent: long-term
Owners: []
RelatedFiles:
    - Path: cmd/goja-perf/phase1_run_command.go
      Note: Benchmark execution pipeline and artifact generation hooks
    - Path: cmd/goja-perf/phase1_types.go
      Note: Run report schema where profiles section should be added
    - Path: cmd/goja-perf/serve_command.go
      Note: Existing HTMX dashboard rendering and route structure
    - Path: cmd/goja-perf/serve_format.go
      Note: Task/benchmark view-model formatting for the UI
    - Path: cmd/goja-perf/serve_streaming.go
      Note: Run lifecycle/polling that new profiles view must coexist with
    - Path: ttmp/2026/02/18/GC-04-ENGINE-FACTORY--implement-enginefactory-for-reusable-runtime-setup/various/profiles/runtime_spawn_profile_summary.yaml
      Note: Example structured profile artifact data contract
ExternalSources: []
Summary: Implementation handoff plan for exposing flamegraph/profile artifacts in the upgraded goja-perf web dashboard.
LastUpdated: 2026-02-18T15:22:00-05:00
WhatFor: Give frontend and backend implementers a shared contract and phased plan to add flamegraph browsing in the current HTMX UI.
WhenToUse: Use when implementing or reviewing flamegraph/profile visualization in goja-perf serve mode.
---


# Flamegraph Web UI Plan

## Goal

Add first-class flamegraph/profile browsing to the upgraded `goja-perf serve`
dashboard so users can inspect CPU profile SVGs, compare baseline vs candidate,
and correlate visuals with benchmark task context.

## Context

- Current dashboard architecture is server-rendered HTML fragments using HTMX:
  - page shell + handlers in `cmd/goja-perf/serve_command.go`
  - background run polling in `cmd/goja-perf/serve_streaming.go`
  - benchmark card formatting in `cmd/goja-perf/serve_format.go`
- Current run reports (`phase1RunReport` / `phase2RunReport`) contain benchmark
  metrics only and no explicit profile artifact section.
- Flamegraph-like outputs already exist as SVG files generated by
  `go tool pprof -svg` and currently live in ticket artifact folders
  (example in `GC-04` profile summary YAML).

## Current-State Analysis

1. Data already available:
- Benchmark summaries are parsed and rendered as per-task cards.
- Profile artifacts can be produced offline and summarized in YAML.

2. Gaps blocking UI display:
- No report schema fields describing profile artifacts.
- No backend endpoint to safely serve local SVG/pprof artifacts.
- No UI region/tabs for profile discovery and comparison.

3. Constraints:
- The app currently favors HTMX fragment responses over SPA JSON APIs.
- File serving must prevent path traversal and accidental repo-wide exposure.
- Flamegraph SVGs can be large; view should support lazy loading.

## Quick Reference

### Recommended Architecture (incremental, low-risk)

1. Keep HTMX + server-rendered fragments (do not replatform UI).
2. Extend run report schema with a `profiles` section.
3. Add dedicated profile artifact endpoints with strict path scoping.
4. Add a "Profiles" panel/tab in existing phase view.

### Proposed YAML/Data Contract

```yaml
profiles:
  generated_at: "2026-02-18T15:12:00-05:00"
  artifacts:
    - id: runtime_spawn_engine_new_cpu_svg
      title: "Runtime Spawn - EngineNew CPU"
      description: "CPU callgraph SVG for EngineNew path"
      phase: "phase1"
      task_id: "p1-runtime-lifecycle"
      benchmark: "BenchmarkRuntimeSpawn/EngineNew_NoCallLog"
      kind: "flamegraph_svg"        # flamegraph_svg | pprof_cpu | pprof_mem | summary
      rel_path: "ttmp/.../engine_new_cpu_5s.svg"
      mime: "image/svg+xml"
      bytes: 182000
      generated_at: "2026-02-18T15:08:10-05:00"
      tags: ["cpu", "baseline"]
  comparisons:
    - id: runtime_spawn_engine_factory_vs_new
      title: "EngineFactory vs EngineNew"
      baseline_artifact_id: runtime_spawn_engine_new_cpu_svg
      candidate_artifact_id: runtime_spawn_engine_factory_cpu_svg
      diff_artifact_id: cpu_diff_5s_svg
      summary_artifact_id: runtime_spawn_enginefactory_vs_new_benchstat
```

### Backend Endpoint Contract

1. `GET /api/profiles/{phase}`
- Response: HTML fragment (aligned with existing HTMX flow).
- Includes artifact table + comparison cards.

2. `GET /api/profile-artifact/{phase}/{artifactID}`
- Serves file content (`image/svg+xml`, `text/plain`, `application/octet-stream`).
- Validates `artifactID` exists in the loaded report manifest.
- Resolves only whitelisted `rel_path` values from manifest.

3. `GET /api/profile-download/{phase}/{artifactID}`
- Same lookup as above, but with download headers.

### Security Rules (must implement)

1. Never accept raw file paths from query params.
2. Resolve path from manifest only.
3. `clean := filepath.Clean(path)` and enforce `strings.HasPrefix(clean, repoRoot)`.
4. Reject non-regular files and missing artifacts with `404`.

### UI Composition Plan

1. Add phase-local secondary nav:
- `Benchmarks` (existing content)
- `Profiles` (new)

2. Profiles fragment layout:
- Top summary row:
  - profile generation time
  - total artifacts
  - total comparisons
- Artifact table columns:
  - title
  - task
  - benchmark
  - type
  - generated timestamp
  - actions (`View`, `Download`)
- Comparison cards:
  - baseline SVG preview link
  - candidate SVG preview link
  - diff SVG preview link
  - optional benchstat/summary text block

3. SVG viewing behavior:
- Default action opens `profile-artifact` in new tab.
- Optional inline viewer via `<object type="image/svg+xml">` only when user expands.

### Phased Implementation Tasks

1. Data model and reporting:
- Add `phaseProfileSection`, `profileArtifact`, `profileComparison` structs.
- Extend `phase1RunReport`/`phase2RunReport` with `Profiles` field.
- Produce profile manifest file during run (or ingest from known summary YAML).

2. Backend serving:
- Add profile-manifest load path to `perfWebApp`.
- Add `/api/profiles/{phase}` and artifact endpoints.
- Add path safety checks and mime detection.

3. UI integration:
- Add Benchmarks/Profiles sub-toggle in current fragment.
- Render profile table and comparison cards from manifest.
- Add loading/empty/error states parallel to report view behavior.

4. Tests:
- Unit tests for manifest parsing and artifact lookup.
- Endpoint tests for 404 and path-traversal rejection.
- Fragment rendering tests for populated and empty profile states.

### Acceptance Criteria

1. User can open Profiles view per phase without leaving the dashboard.
2. At least one comparison card shows baseline/candidate/diff links.
3. Artifact endpoints only serve manifest-backed files under repo root.
4. Existing benchmark table UX remains unchanged when no profiles exist.
5. `go test ./cmd/goja-perf` passes with new tests.

### Risks and Mitigations

1. Risk: Very large SVGs degrade page performance.
- Mitigation: lazy-expand inline view; default to external tab.

2. Risk: Report/profile files generated in different runs become stale.
- Mitigation: embed `generated_at` + run ID and show mismatch warnings.

3. Risk: Endpoint security regressions.
- Mitigation: explicit traversal tests and manifest-only lookup.

## Usage Examples

### Example Handoff to Frontend Developer

1. Implement the Profiles tab and fragment rendering in
   `cmd/goja-perf/serve_command.go`.
2. Reuse existing task card style tokens from `indexTemplate`.
3. Consume profile data from `report.Profiles` only.

### Example Backend Development Order

1. Add report schema types in `cmd/goja-perf/phase1_types.go`.
2. Load profile manifests in `handleReport` flow.
3. Add artifact-serving handlers and tests.
4. Wire HTMX controls for profile fragment switching.

## Related

- `ttmp/2026/02/18/GC-04-ENGINE-FACTORY--implement-enginefactory-for-reusable-runtime-setup/reference/01-implementation-plan.md`
- `ttmp/2026/02/18/GJ-02-BETTER-UI--better-performance-measurement-dashboard-ui/index.md`
